{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "768b1074",
   "metadata": {
    "id": "768b1074"
   },
   "source": [
    "# Fine-tune Qwen3-1.7B with LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pqOBrzvvz6pB",
   "metadata": {
    "id": "pqOBrzvvz6pB"
   },
   "source": [
    "This notebook guide provides a comprehensive overview of using the `transformers` Python package to efficiently train a custom model. It covers the following techniques:\n",
    "\n",
    "1. Utilizing model, tokenizer, and dataset loading functionalities from Hugging Face.\n",
    "2. Loading and preprocessing custom dataset.\n",
    "3. Training the model with QLoRA (4-bit quantization + LoRA).\n",
    "4. Evaluating the model's performance on validation set.\n",
    "5. Saving your custom model and preparing it for deployment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u4AcE7nU1qQI",
   "metadata": {
    "id": "u4AcE7nU1qQI"
   },
   "source": [
    "## Preliminary Preparation\n",
    "\n",
    "Before proceeding with model training, ensure your environment is properly configured by following these steps:\n",
    "\n",
    "1. Install the necessary Python packages.\n",
    "2. Import the required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6c6af6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:15:07.736857Z",
     "iopub.status.busy": "2025-11-19T00:15:07.736589Z",
     "iopub.status.idle": "2025-11-19T00:17:45.595093Z",
     "shell.execute_reply": "2025-11-19T00:17:45.594054Z",
     "shell.execute_reply.started": "2025-11-19T00:15:07.736835Z"
    },
    "id": "6f6c6af6",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# å®‰è£…æ ¸å¿ƒä¾èµ–\n",
    "!pip install -q protobuf==3.20.3\n",
    "!pip install -q h5py typing-extensions wheel\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U transformers==4.53\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q datasets\n",
    "!pip install -q matplotlib\n",
    "!pip install -q pandas\n",
    "!pip install -q fschat\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"âœ… ä¾èµ–å®‰è£…å®Œæˆï¼\")\n",
    "print(\"âš ï¸  å¦‚æœé‡åˆ°protobufè­¦å‘Šï¼Œè¯·é‡å¯kernelåç»§ç»­è¿è¡Œ\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_T66f30GsT0t",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:17:45.597168Z",
     "iopub.status.busy": "2025-11-19T00:17:45.596865Z",
     "iopub.status.idle": "2025-11-19T00:17:45.762346Z",
     "shell.execute_reply": "2025-11-19T00:17:45.761607Z",
     "shell.execute_reply.started": "2025-11-19T00:17:45.597143Z"
    },
    "id": "_T66f30GsT0t",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9726766",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:17:45.763694Z",
     "iopub.status.busy": "2025-11-19T00:17:45.763416Z",
     "iopub.status.idle": "2025-11-19T00:17:45.769627Z",
     "shell.execute_reply": "2025-11-19T00:17:45.769010Z",
     "shell.execute_reply.started": "2025-11-19T00:17:45.763669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set Hugging Face cache directory to Kaggle working directory\n",
    "import os\n",
    "\n",
    "cache_dir = '/kaggle/working/huggingface_cache'\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "os.environ['HF_HOME'] = cache_dir\n",
    "os.environ['TRANSFORMERS_CACHE'] = f'{cache_dir}/transformers'\n",
    "os.environ['HF_DATASETS_CACHE'] = f'{cache_dir}/datasets'\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(f\"Hugging Face ç¼“å­˜ç›®å½•å·²è®¾ç½®ä¸º: {cache_dir}\")\n",
    "print(f\"æ¨¡å‹å’Œæ•°æ®é›†å°†ä¿å­˜åˆ°Kaggleå·¥ä½œç›®å½•\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a194ac33",
   "metadata": {
    "id": "a194ac33"
   },
   "source": [
    "## Load Pre-trained model and tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uGLMe7qCXQGQ",
   "metadata": {
    "id": "uGLMe7qCXQGQ"
   },
   "source": [
    "First let's load the model we are going to use - Qwen3-1.7B! This is a smaller model suitable for efficient fine-tuning. The model is available at https://huggingface.co/Qwen/Qwen3-1.7B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wye4Wm9rXgFF",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:17:45.770835Z",
     "iopub.status.busy": "2025-11-19T00:17:45.770440Z",
     "iopub.status.idle": "2025-11-19T00:19:10.925491Z",
     "shell.execute_reply": "2025-11-19T00:19:10.924737Z",
     "shell.execute_reply.started": "2025-11-19T00:17:45.770817Z"
    },
    "id": "wye4Wm9rXgFF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import os\n",
    "\n",
    "# Quantization type (fp4 or nf4), According to QLoRA paper, for training 4-bit base models (e.g. using LoRA adapters) one should use\n",
    "bnb_4bit_quant_type = \"nf4\"\n",
    "\n",
    "# Activate nested quantization for 4-bit base models (double quantization)\n",
    "use_nested_quant = True\n",
    "\n",
    "model_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# Check if model is already cached in Kaggle\n",
    "# Hugging Face stores models in snapshots directory\n",
    "model_cache_path = os.path.join(\n",
    "    os.environ.get('TRANSFORMERS_CACHE', ''),\n",
    "    'models--Qwen--Qwen3-1_7B'\n",
    ")\n",
    "\n",
    "def check_model_cached(cache_path):\n",
    "    \"\"\"Check if model files actually exist in cache\"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        return False\n",
    "    \n",
    "    # Check for snapshots directory\n",
    "    snapshots_path = os.path.join(cache_path, 'snapshots')\n",
    "    if not os.path.exists(snapshots_path):\n",
    "        return False\n",
    "    \n",
    "    # Check if any snapshot directory has ALL model weight files\n",
    "    try:\n",
    "        snapshot_dirs = os.listdir(snapshots_path)\n",
    "        if not snapshot_dirs:\n",
    "            return False\n",
    "        \n",
    "        # Check the most recent snapshot for model files\n",
    "        for snapshot_dir in snapshot_dirs:\n",
    "            snapshot_full_path = os.path.join(snapshots_path, snapshot_dir)\n",
    "            if os.path.isdir(snapshot_full_path):\n",
    "                files = os.listdir(snapshot_full_path)\n",
    "                # Look for safetensors or bin files\n",
    "                model_files = [f for f in files if f.endswith('.safetensors') or f.endswith('.bin')]\n",
    "                \n",
    "                # Qwen3-1.7B should have model files\n",
    "                # Check if we have at least one complete model file > 100MB to verify it's actually downloaded\n",
    "                for model_file in model_files:\n",
    "                    file_path = os.path.join(snapshot_full_path, model_file)\n",
    "                    if os.path.exists(file_path):\n",
    "                        file_size = os.path.getsize(file_path)\n",
    "                        # If we find a large model file (> 100MB), cache is likely complete\n",
    "                        if file_size > 100 * 1024 * 1024:  # 100MB\n",
    "                            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸  æ£€æŸ¥ç¼“å­˜æ—¶å‡ºé”™: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"=\" * 70)\n",
    "if check_model_cached(model_cache_path):\n",
    "    print(\"âœ… æ£€æµ‹åˆ°å·²ç¼“å­˜å®Œæ•´æ¨¡å‹\")\n",
    "    print(f\"ğŸ“‚ ç¼“å­˜ä½ç½®: {model_cache_path}\")\n",
    "    print(\"âš¡ å°†ä»ç¼“å­˜åŠ è½½ï¼Œæ— éœ€é‡æ–°ä¸‹è½½\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ£€æµ‹åˆ°å®Œæ•´æ¨¡å‹ç¼“å­˜\")\n",
    "    print(\"ğŸ“¥ é¦–æ¬¡ä¸‹è½½æ¨¡å‹ (çº¦ 3-4 GB)ï¼Œè¯·è€å¿ƒç­‰å¾…...\")\n",
    "    print(f\"ğŸ’¾ æ¨¡å‹å°†ä¿å­˜åˆ°: {model_cache_path}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Kaggle GPUæ”¯æŒ4-bité‡åŒ–\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=use_nested_quant,\n",
    "    bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ”„ åŠ è½½ Tokenizer...\")\n",
    "\n",
    "# Get the cache directory from environment variable\n",
    "transformers_cache_dir = os.environ.get('TRANSFORMERS_CACHE', '')\n",
    "\n",
    "# Check if tokenizer is cached\n",
    "def check_tokenizer_cached(cache_path):\n",
    "    \"\"\"Check if tokenizer files exist in cache\"\"\"\n",
    "    if not os.path.exists(cache_path):\n",
    "        return False\n",
    "    \n",
    "    snapshots_path = os.path.join(cache_path, 'snapshots')\n",
    "    if not os.path.exists(snapshots_path):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        snapshot_dirs = os.listdir(snapshots_path)\n",
    "        if not snapshot_dirs:\n",
    "            return False\n",
    "        \n",
    "        for snapshot_dir in snapshot_dirs:\n",
    "            snapshot_full_path = os.path.join(snapshots_path, snapshot_dir)\n",
    "            if os.path.isdir(snapshot_full_path):\n",
    "                files = os.listdir(snapshot_full_path)\n",
    "                # Look for tokenizer files\n",
    "                has_tokenizer = any(f.startswith('tokenizer') for f in files)\n",
    "                if has_tokenizer:\n",
    "                    return True\n",
    "        return False\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "if check_tokenizer_cached(model_cache_path):\n",
    "    print(\"âœ… Tokenizer å·²ç¼“å­˜ï¼Œä»æœ¬åœ°åŠ è½½\")\n",
    "else:\n",
    "    print(\"ğŸ“¥ é¦–æ¬¡ä¸‹è½½ Tokenizer æ–‡ä»¶...\")\n",
    "\n",
    "# æ˜¾å¼æŒ‡å®š cache_dir\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, cache_dir=transformers_cache_dir)\n",
    "\n",
    "# Fix: Set pad_token to avoid attention_mask warnings\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    print(f\"è®¾ç½® pad_token = eos_token ({tokenizer.pad_token})\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"ğŸ”„ åŠ è½½æ¨¡å‹...\")\n",
    "# Kaggle GPUæ”¯æŒé‡åŒ–å’Œdevice_map\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map={\"\":0},\n",
    "    cache_dir=transformers_cache_dir\n",
    ")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Resize token embeddings to match tokenizer\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "print(f\"\\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼Œvocabulary size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c254e5db",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "This section covers the complete data pipeline:\n",
    "1. Loading the custom dataset from local files\n",
    "2. Converting data to conversation format\n",
    "3. Creating Hugging Face Dataset\n",
    "4. Splitting into train/validation sets\n",
    "5. Implementing custom dataset class for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14726c09",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T02:00:21.648450Z",
     "iopub.status.busy": "2025-11-19T02:00:21.648181Z",
     "iopub.status.idle": "2025-11-19T02:00:21.762403Z",
     "shell.execute_reply": "2025-11-19T02:00:21.761579Z",
     "shell.execute_reply.started": "2025-11-19T02:00:21.648432Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# åœ¨Kaggleä¸­æŸ¥æ‰¾æ•°æ®é›†æ–‡ä»¶\n",
    "# æ”¯æŒå¤šç§å¯èƒ½çš„æ•°æ®é›†è·¯å¾„\n",
    "possible_paths = [\n",
    "    \"/kaggle/input/nlp-final/generated_dataset.json\",      # æœ€å¯èƒ½çš„è·¯å¾„\n",
    "    \"/kaggle/input/nlp-final/data/generated_dataset.json\",  # å¦‚æœæ–‡ä»¶åœ¨dataå­ç›®å½•\n",
    "    \"/kaggle/input/nlp-final/*/generated_dataset.json\",     # é€šé…ç¬¦æœç´¢\n",
    "    \"/kaggle/input/*/generated_dataset.json\",               # æ›´å¹¿æ³›çš„æœç´¢\n",
    "    \"data/generated_dataset.json\",                          # æœ¬åœ°æµ‹è¯•è·¯å¾„\n",
    "    \"generated_dataset.json\"                                # å½“å‰ç›®å½•\n",
    "]\n",
    "\n",
    "dataset_file = None\n",
    "for path in possible_paths:\n",
    "    if '*' in path:\n",
    "        # ä½¿ç”¨globæŸ¥æ‰¾\n",
    "        matches = glob.glob(path)\n",
    "        if matches:\n",
    "            dataset_file = matches[0]\n",
    "            break\n",
    "    elif os.path.exists(path):\n",
    "        dataset_file = path\n",
    "        break\n",
    "\n",
    "print(\"=\" * 80)\n",
    "if dataset_file:\n",
    "    print(f\"âœ… æ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶: {dataset_file}\")\n",
    "    print(\"=\" * 80)\n",
    "    with open(dataset_file, 'r', encoding='utf-8') as f:\n",
    "        raw_data = json.load(f)\n",
    "    print(f\"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(raw_data)} æ¡\")\n",
    "    print(f\"\\nğŸ“ ç¤ºä¾‹æ•°æ®:\")\n",
    "    print(f\"   é—®é¢˜: {raw_data[0]['question'][:50]}...\")\n",
    "    print(f\"   ç­”æ¡ˆ: {str(raw_data[0]['answer'])[:80]}...\")\n",
    "else:\n",
    "    print(\"âš ï¸  æœªæ‰¾åˆ°æ•°æ®é›†æ–‡ä»¶ generated_dataset.json\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nğŸ’¡ æœç´¢çš„è·¯å¾„:\")\n",
    "    for path in possible_paths:\n",
    "        print(f\"   â€¢ {path}\")\n",
    "    print(\"\\nâŒ è¯·ç¡®ä¿åœ¨Kaggle Notebookä¸­æ·»åŠ  'nlp-final' dataset\")\n",
    "    raise FileNotFoundError(\"æ— æ³•æ‰¾åˆ° generated_dataset.json æ–‡ä»¶\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b259774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:19:12.884552Z",
     "iopub.status.busy": "2025-11-19T00:19:12.884207Z",
     "iopub.status.idle": "2025-11-19T00:19:15.461101Z",
     "shell.execute_reply": "2025-11-19T00:19:15.460204Z",
     "shell.execute_reply.started": "2025-11-19T00:19:12.884522Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Convert the dataset to the required format\n",
    "# The dataset has 'question' and 'answer' fields, where answer is a JSON array\n",
    "# We need to convert answer to string format for training\n",
    "converted_data = []\n",
    "for item in raw_data:\n",
    "    # Convert answer (JSON array) to string format\n",
    "    answer_str = json.dumps(item['answer'], ensure_ascii=False)\n",
    "    converted_data.append({\n",
    "        \"conversations\": [\n",
    "            {\"role\": \"human\", \"value\": item['question']},\n",
    "            {\"role\": \"gpt\", \"value\": answer_str}\n",
    "        ]\n",
    "    })\n",
    "\n",
    "print(f\"âœ… æ•°æ®è½¬æ¢å®Œæˆ: {len(converted_data)} æ¡\")\n",
    "print(f\"\\nğŸ“ è½¬æ¢åç¤ºä¾‹:\")\n",
    "print(f\"   é—®é¢˜: {converted_data[0]['conversations'][0]['value'][:50]}...\")\n",
    "print(f\"   ç­”æ¡ˆ: {converted_data[0]['conversations'][1]['value'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5a5ce5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:19:15.462260Z",
     "iopub.status.busy": "2025-11-19T00:19:15.462018Z",
     "iopub.status.idle": "2025-11-19T00:41:44.444378Z",
     "shell.execute_reply": "2025-11-19T00:41:44.443484Z",
     "shell.execute_reply.started": "2025-11-19T00:19:15.462242Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Display data sample information\n",
    "print(f\"æ•°æ®æ ·æœ¬é¢„è§ˆï¼ˆå…± {len(converted_data)} æ¡ï¼‰ï¼š\")\n",
    "print(f\"   é—®é¢˜: {converted_data[0]['conversations'][0]['value'][:50]}...\")\n",
    "print(f\"   ç­”æ¡ˆ: {converted_data[0]['conversations'][1]['value'][:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125fc0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_from_disk\n",
    "import os\n",
    "\n",
    "# å®šä¹‰æ•°æ®é›†çš„ä¿å­˜è·¯å¾„\n",
    "dataset_path = \"/kaggle/working/huggingface_cache/custom_dataset\"\n",
    "\n",
    "# æ£€æŸ¥æ˜¯å¦å·²æœ‰ä¿å­˜çš„æ•°æ®é›†\n",
    "if os.path.exists(dataset_path):\n",
    "    print(\"=\" * 80)\n",
    "    print(\"âœ… å‘ç°å·²ä¿å­˜çš„æ•°æ®é›†ï¼Œç›´æ¥åŠ è½½...\")\n",
    "    print(f\"ğŸ“‚ ä½ç½®: {dataset_path}\")\n",
    "    dataset = load_from_disk(dataset_path)\n",
    "    print(f\"âœ… æ•°æ®é›†åŠ è½½å®Œæˆ: {len(dataset)} æ¡\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"ğŸ“¦ åˆ›å»º Hugging Face Dataset...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create Hugging Face Dataset from converted data\n",
    "    dataset = Dataset.from_list(converted_data)\n",
    "    print(f\"âœ… æ•°æ®é›†åˆ›å»ºå®Œæˆ: {len(dataset)} æ¡\")\n",
    "    \n",
    "    # ğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°ç£ç›˜\n",
    "    print(f\"\\nğŸ’¾ ä¿å­˜æ•°æ®é›†åˆ°: {dataset_path}\")\n",
    "    os.makedirs(os.path.dirname(dataset_path), exist_ok=True)\n",
    "    dataset.save_to_disk(dataset_path)\n",
    "    print(\"âœ… æ•°æ®é›†å·²ä¿å­˜ï¼ä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥åŠ è½½\")\n",
    "    print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1afd80b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# æŒ‰æ¯”ä¾‹åˆ’åˆ†æ•°æ®é›†ï¼ˆæ›´çµæ´»ï¼Œè‡ªåŠ¨é€‚åº”æ•°æ®é›†å¤§å°ï¼‰\n",
    "train_ratio = 0.8   # 80% ç”¨äºè®­ç»ƒ\n",
    "\n",
    "total_size = len(dataset)\n",
    "train_size = int(total_size * train_ratio)\n",
    "val_size = total_size - train_size  # ç¡®ä¿æ€»å’Œç­‰äº total_sizeï¼Œé¿å…èˆå…¥è¯¯å·®\n",
    "\n",
    "print(f\"ğŸ“Š æ•°æ®é›†åˆ’åˆ†:\")\n",
    "print(f\"   æ€»æ•°æ®é‡: {total_size} æ¡\")\n",
    "print(f\"   è®­ç»ƒé›†: {train_size} æ¡ ({train_size/total_size*100:.1f}%)\")\n",
    "print(f\"   éªŒè¯é›†: {val_size} æ¡ ({val_size/total_size*100:.1f}%)\")\n",
    "\n",
    "# åªåˆ†æˆä¸¤éƒ¨åˆ†ï¼šè®­ç»ƒé›†å’ŒéªŒè¯é›†\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42d2670",
   "metadata": {},
   "source": [
    "### Customized Dataset Class\n",
    "\n",
    "Create a specialized dataset class named \"InstructDataset\" designed to handle our custom dataset format with proper tokenization and label masking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cd220c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, copy\n",
    "import transformers\n",
    "from typing import Dict, Sequence, List\n",
    "from dataclasses import dataclass\n",
    "from torch.utils.data import Dataset\n",
    "from fastchat.conversation import get_conv_template\n",
    "\n",
    "IGNORE_INDEX = -100\n",
    "DEFAULT_PAD_TOKEN = \"<pad>\"\n",
    "DEFAULT_BOS_TOKEN = \"<s>\"\n",
    "DEFAULT_EOS_TOKEN = \"</s>\"\n",
    "DEFAULT_UNK_TOKEN = \"<unk>\"\n",
    "default_conversation = get_conv_template('phoenix')\n",
    "\n",
    "class InstructDataset(Dataset):\n",
    "    def __init__(self, data: Sequence, tokenizer: transformers.PreTrainedTokenizer) -> None:\n",
    "        super().__init__()\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index) -> Dict[str, torch.Tensor]:\n",
    "        sources = self.data[index]\n",
    "        if isinstance(index, int):\n",
    "            sources = [sources]\n",
    "        data_dict = preprocess([e['conversations'] for e in sources], self.tokenizer)\n",
    "        if isinstance(index, int):\n",
    "            data_dict = dict(input_ids=data_dict[\"input_ids\"][0], labels=data_dict[\"labels\"][0])\n",
    "        return data_dict\n",
    "\n",
    "def preprocess(\n",
    "        sources: Sequence[str],\n",
    "        tokenizer: transformers.PreTrainedTokenizer,\n",
    "        max_length=1024\n",
    ") -> Dict:\n",
    "    # add end signal and concatenate together\n",
    "    conversations = []\n",
    "    intermediates = []\n",
    "    for source in sources:\n",
    "        header = f\"{default_conversation.system_message}\"\n",
    "        conversation, intermediate = _add_speaker_and_signal(header, source)\n",
    "        conversations.append(conversation)\n",
    "        intermediates.append(intermediate)\n",
    "\n",
    "    # tokenize conversations\n",
    "    conversations_tokenized = _tokenize_fn(conversations, tokenizer)\n",
    "    input_ids = conversations_tokenized[\"input_ids\"]\n",
    "    targets = copy.deepcopy(input_ids)\n",
    "\n",
    "    # keep only machine responses as targets\n",
    "    assert len(targets) == len(intermediates)\n",
    "    for target, inters in zip(targets, intermediates):\n",
    "        mask = torch.zeros_like(target, dtype=torch.bool)\n",
    "        for inter in inters:\n",
    "            tokenized = _tokenize_fn(inter, tokenizer)\n",
    "            start_idx = tokenized[\"input_ids\"][0].size(0) - 1\n",
    "            end_idx = tokenized[\"input_ids\"][1].size(0)\n",
    "            mask[start_idx:end_idx] = True\n",
    "        target[~mask] = IGNORE_INDEX\n",
    "\n",
    "    input_ids = input_ids[:max_length]\n",
    "    targets = targets[:max_length]\n",
    "    return dict(input_ids=input_ids, labels=targets)\n",
    "\n",
    "def _add_speaker_and_signal(header, source, get_conversation=True):\n",
    "    BEGIN_SIGNAL = DEFAULT_BOS_TOKEN\n",
    "    END_SIGNAL = DEFAULT_EOS_TOKEN\n",
    "    conversation = header\n",
    "    intermediate = []\n",
    "    for sentence in source:\n",
    "        from_str = sentence[\"role\"]\n",
    "        if from_str.lower() == \"human\":\n",
    "            from_str = default_conversation.roles[0]\n",
    "        elif from_str.lower() == \"gpt\":\n",
    "            from_str = default_conversation.roles[1]\n",
    "        else:\n",
    "            from_str = 'unknown'\n",
    "        # store the string w/o and w/ the response\n",
    "        value = (from_str + \": \" + BEGIN_SIGNAL + sentence[\"value\"] + END_SIGNAL)\n",
    "        if sentence[\"role\"].lower() == \"gpt\":\n",
    "            start = conversation + from_str + \": \" + BEGIN_SIGNAL\n",
    "            end = conversation + value\n",
    "            intermediate.append([start, end])\n",
    "        if get_conversation:\n",
    "            conversation += value\n",
    "    return conversation, intermediate\n",
    "\n",
    "def _tokenize_fn(strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "    tokenized_list = [\n",
    "        tokenizer(\n",
    "            text,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=\"longest\",\n",
    "            max_length=tokenizer.model_max_length,\n",
    "            truncation=True,\n",
    "        ) for text in strings\n",
    "    ]\n",
    "    input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "    input_ids_lens = labels_lens = [\n",
    "        tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item()\n",
    "        for tokenized in tokenized_list\n",
    "    ]\n",
    "    return dict(\n",
    "        input_ids=input_ids,\n",
    "        labels=labels,\n",
    "        input_ids_lens=input_ids_lens,\n",
    "        labels_lens=labels_lens,\n",
    "    )\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object):\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids,\n",
    "            batch_first=True,\n",
    "            padding_value=self.tokenizer.pad_token_id)\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c416832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training and validation datasets using the custom dataset class\n",
    "train_dataset = InstructDataset(train_dataset, tokenizer)\n",
    "val_dataset = InstructDataset(val_dataset, tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3eb3055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debug: Check data format\n",
    "sample_data = train_dataset[1]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Debuging: \")\n",
    "print(sample_data)\n",
    "print(\"-\" * 80)\n",
    "print(f\"input_ids:\\n{tokenizer.decode(sample_data['input_ids'])}\")\n",
    "# Filter out IGNORE_INDEX before decoding labels\n",
    "z = [token for token in sample_data['labels'] if token != IGNORE_INDEX]\n",
    "print(\"-\" * 80)\n",
    "print(f\"labels:\\n{tokenizer.decode(z)}\")\n",
    "print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "BFNSifwxX8Ju",
   "metadata": {
    "id": "BFNSifwxX8Ju"
   },
   "source": [
    "## Model Preparation\n",
    "\n",
    "Then we have to apply some preprocessing to the model to prepare it for training. For that use the `prepare_model_for_kbit_training` method from PEFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpWUSDliX-e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:41:44.451913Z",
     "iopub.status.busy": "2025-11-19T00:41:44.451423Z",
     "iopub.status.idle": "2025-11-19T00:41:44.708565Z",
     "shell.execute_reply": "2025-11-19T00:41:44.707975Z",
     "shell.execute_reply.started": "2025-11-19T00:41:44.451889Z"
    },
    "id": "bpWUSDliX-e2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agMER78BYbxp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:41:44.709693Z",
     "iopub.status.busy": "2025-11-19T00:41:44.709441Z",
     "iopub.status.idle": "2025-11-19T00:41:44.713807Z",
     "shell.execute_reply": "2025-11-19T00:41:44.713202Z",
     "shell.execute_reply.started": "2025-11-19T00:41:44.709674Z"
    },
    "id": "agMER78BYbxp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZWzMDILnYecp",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:41:44.714669Z",
     "iopub.status.busy": "2025-11-19T00:41:44.714478Z",
     "iopub.status.idle": "2025-11-19T00:41:44.969519Z",
     "shell.execute_reply": "2025-11-19T00:41:44.968860Z",
     "shell.execute_reply.started": "2025-11-19T00:41:44.714651Z"
    },
    "id": "ZWzMDILnYecp",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "# You can try differnt parameter-effient strategy for model trianing, for more info, please check https://github.com/huggingface/peft\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=8,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631c6fd1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T02:09:15.173309Z",
     "iopub.status.busy": "2025-11-19T02:09:15.173019Z",
     "iopub.status.idle": "2025-11-19T02:09:29.468168Z",
     "shell.execute_reply": "2025-11-19T02:09:29.467284Z",
     "shell.execute_reply.started": "2025-11-19T02:09:15.173292Z"
    },
    "id": "631c6fd1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastchat.conversation import get_conv_template\n",
    "device = \"cuda\"\n",
    "model.eval()\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    outputs = model.generate(input_ids, do_sample=False, max_new_tokens=2048)\n",
    "    return tokenizer.decode(*outputs, skip_special_tokens=True)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False  # ç¦ç”¨ Qwen3 çš„ thinking æ¨¡å¼\n",
    ")\n",
    "response = generate(text)\n",
    "print(\"-\"*80)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bd2f2fa",
   "metadata": {
    "id": "4bd2f2fa"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479baff4",
   "metadata": {
    "id": "479baff4"
   },
   "source": [
    "### General Training Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77035f45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T00:42:06.867670Z",
     "iopub.status.busy": "2025-11-19T00:42:06.867453Z",
     "iopub.status.idle": "2025-11-19T00:42:06.920698Z",
     "shell.execute_reply": "2025-11-19T00:42:06.919993Z",
     "shell.execute_reply.started": "2025-11-19T00:42:06.867648Z"
    },
    "id": "77035f45",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Set training parameters\n",
    "training_arguments = transformers.TrainingArguments(\n",
    "    output_dir=\"./checkpoints\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=2,\n",
    "    optim='paged_adamw_32bit',\n",
    "    save_steps=0,\n",
    "    logging_steps=1,\n",
    "    eval_steps=50,  # æ¯50æ­¥è¯„ä¼°ä¸€æ¬¡éªŒè¯é›†\n",
    "    eval_strategy=\"steps\",  # å¯ç”¨éªŒè¯é›†è¯„ä¼°ï¼ˆæ–°ç‰ˆæœ¬ä½¿ç”¨ eval_strategyï¼‰\n",
    "    learning_rate=2e-7,\n",
    "    weight_decay=0.001,\n",
    "    max_steps=-1,\n",
    "    warmup_ratio=0.03,\n",
    "    group_by_length=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"none\",\n",
    "    load_best_model_at_end=False,  # ä¸åŠ è½½æœ€ä½³æ¨¡å‹ï¼ˆèŠ‚çœå†…å­˜ï¼‰\n",
    "    save_total_limit=1  # åªä¿ç•™æœ€æ–°çš„checkpoint\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f96057",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T00:42:06.921726Z",
     "iopub.status.busy": "2025-11-19T00:42:06.921486Z",
     "iopub.status.idle": "2025-11-19T00:42:08.137615Z",
     "shell.execute_reply": "2025-11-19T00:42:08.136943Z",
     "shell.execute_reply.started": "2025-11-19T00:42:06.921710Z"
    },
    "id": "77f96057",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.train()\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    args=training_arguments,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=data_collator\n",
    ")\n",
    "# ä¸ºPEFTæ¨¡å‹è®¾ç½®label_namesï¼Œé¿å…è­¦å‘Š\n",
    "trainer.label_names = [\"labels\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4f39e0",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T00:42:08.138668Z",
     "iopub.status.busy": "2025-11-19T00:42:08.138342Z",
     "iopub.status.idle": "2025-11-19T00:42:08.148831Z",
     "shell.execute_reply": "2025-11-19T00:42:08.147995Z",
     "shell.execute_reply.started": "2025-11-19T00:42:08.138648Z"
    },
    "id": "3a4f39e0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe99d72e",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T00:42:08.150080Z",
     "iopub.status.busy": "2025-11-19T00:42:08.149658Z",
     "iopub.status.idle": "2025-11-19T01:26:12.846997Z",
     "shell.execute_reply": "2025-11-19T01:26:12.845999Z",
     "shell.execute_reply.started": "2025-11-19T00:42:08.150046Z"
    },
    "id": "fe99d72e",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af3f9bb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:26:12.848116Z",
     "iopub.status.busy": "2025-11-19T01:26:12.847823Z",
     "iopub.status.idle": "2025-11-19T01:26:15.086119Z",
     "shell.execute_reply": "2025-11-19T01:26:15.085379Z",
     "shell.execute_reply.started": "2025-11-19T01:26:12.848091Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Plot training and validation loss curves with dual y-axes\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ“Š ç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ›²çº¿ï¼ˆåŒYè½´ï¼‰...\")\n",
    "\n",
    "# Extract loss history from trainer\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and validation logs\n",
    "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# Extract data\n",
    "train_steps = [log['step'] for log in train_logs]\n",
    "train_loss = [log['loss'] for log in train_logs]\n",
    "\n",
    "eval_steps = [log['step'] for log in eval_logs]\n",
    "eval_loss = [log['eval_loss'] for log in eval_logs]\n",
    "\n",
    "print(f\"âœ… å·²æå– {len(train_logs)} ä¸ªè®­ç»ƒlossè®°å½•\")\n",
    "print(f\"âœ… å·²æå– {len(eval_logs)} ä¸ªéªŒè¯lossè®°å½•\")\n",
    "\n",
    "# Create the plot with dual y-axes\n",
    "fig, ax1 = plt.subplots(figsize=(12, 7))\n",
    "\n",
    "# Left y-axis: Training Loss\n",
    "color_train = 'tab:blue'\n",
    "ax1.set_xlabel('Steps', fontsize=13, fontweight='bold')\n",
    "ax1.set_ylabel('Training Loss', fontsize=13, fontweight='bold', color=color_train)\n",
    "line1 = ax1.plot(train_steps, train_loss, 'o-', color=color_train, label='Training Loss', \n",
    "                 linewidth=2, markersize=4, alpha=0.8)\n",
    "ax1.tick_params(axis='y', labelcolor=color_train)\n",
    "ax1.grid(True, alpha=0.3, linestyle='--')\n",
    "\n",
    "# Right y-axis: Validation Loss\n",
    "if eval_loss:  # Only plot if validation loss exists\n",
    "    ax2 = ax1.twinx()  # Create a second y-axis sharing the same x-axis\n",
    "    color_val = 'tab:red'\n",
    "    ax2.set_ylabel('Validation Loss', fontsize=13, fontweight='bold', color=color_val)\n",
    "    line2 = ax2.plot(eval_steps, eval_loss, 's-', color=color_val, label='Validation Loss', \n",
    "                     linewidth=2, markersize=6, alpha=0.9)\n",
    "    ax2.tick_params(axis='y', labelcolor=color_val)\n",
    "    \n",
    "    # Combine legends from both axes\n",
    "    lines = line1 + line2\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax1.legend(lines, labels, fontsize=12, loc='best', framealpha=0.9)\n",
    "else:\n",
    "    ax1.legend(fontsize=12, loc='best', framealpha=0.9)\n",
    "\n",
    "plt.title('Training and Validation Loss Curves (Dual Y-Axes)', fontsize=15, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "output_plot_path = '/kaggle/working/lora_models/qwen3_1_7b_custom/loss_curve.png'\n",
    "os.makedirs(os.path.dirname(output_plot_path), exist_ok=True)\n",
    "plt.savefig(output_plot_path, dpi=500, bbox_inches='tight')\n",
    "print(f\"âœ… Loss curve saved to: {output_plot_path}\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ğŸ“ˆ Loss Statistics:\")\n",
    "print(f\"   åˆå§‹è®­ç»ƒLoss: {train_loss[0]:.4f}\")\n",
    "print(f\"   æœ€ç»ˆè®­ç»ƒLoss: {train_loss[-1]:.4f}\")\n",
    "print(f\"   Lossä¸‹é™: {train_loss[0] - train_loss[-1]:.4f} ({(1 - train_loss[-1]/train_loss[0])*100:.2f}%)\")\n",
    "if eval_loss:\n",
    "    print(f\"   åˆå§‹éªŒè¯Loss: {eval_loss[0]:.4f}\")\n",
    "    print(f\"   æœ€ç»ˆéªŒè¯Loss: {eval_loss[-1]:.4f}\")\n",
    "    print(f\"   éªŒè¯Lossä¸‹é™: {eval_loss[0] - eval_loss[-1]:.4f} ({(1 - eval_loss[-1]/eval_loss[0])*100:.2f}%)\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fgrJkL6mwGbt",
   "metadata": {
    "id": "fgrJkL6mwGbt"
   },
   "source": [
    "Once the training is completed, we can evaluate our model and get its perplexity on the validation set like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_5_G78x_wF4w",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T01:26:15.087193Z",
     "iopub.status.busy": "2025-11-19T01:26:15.086916Z",
     "iopub.status.idle": "2025-11-19T01:28:28.003043Z",
     "shell.execute_reply": "2025-11-19T01:28:28.002378Z",
     "shell.execute_reply.started": "2025-11-19T01:26:15.087176Z"
    },
    "id": "_5_G78x_wF4w",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b889f01",
   "metadata": {
    "id": "3b889f01"
   },
   "source": [
    "## Save Trained LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24719a28",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "execution": {
     "iopub.execute_input": "2025-11-19T01:28:28.004095Z",
     "iopub.status.busy": "2025-11-19T01:28:28.003814Z",
     "iopub.status.idle": "2025-11-19T01:28:37.452422Z",
     "shell.execute_reply": "2025-11-19T01:28:37.451555Z",
     "shell.execute_reply.started": "2025-11-19T01:28:28.004073Z"
    },
    "id": "24719a28",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Save trained LoRA model and tokenizer\n",
    "output_path = \"/kaggle/working/lora_models/qwen3_1_7b_custom\"\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "trainer.save_model(output_path)\n",
    "# ğŸ’¾ åŒæ—¶ä¿å­˜tokenizerï¼Œç¡®ä¿åŠ è½½æ—¶è¯æ±‡è¡¨å¤§å°ä¸€è‡´\n",
    "tokenizer.save_pretrained(output_path)\n",
    "print(f\"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {output_path}\")\n",
    "print(f\"âœ… Tokenizerå·²ä¿å­˜åˆ°: {output_path}\")\n",
    "print(f\"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Jgi46OdfvRkM",
   "metadata": {
    "id": "Jgi46OdfvRkM"
   },
   "source": [
    "### Test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQWOFwHqvUFZ",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:28:37.453866Z",
     "iopub.status.busy": "2025-11-19T01:28:37.453545Z",
     "iopub.status.idle": "2025-11-19T01:28:54.925131Z",
     "shell.execute_reply": "2025-11-19T01:28:54.924464Z",
     "shell.execute_reply.started": "2025-11-19T01:28:37.453838Z"
    },
    "id": "eQWOFwHqvUFZ",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from fastchat.conversation import get_conv_template\n",
    "device = \"cuda\"\n",
    "model.eval()\n",
    "@torch.no_grad()\n",
    "def generate(prompt):\n",
    "    input_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors='pt').to(device)\n",
    "    outputs = trainer.model.generate(input_ids, do_sample=False, max_new_tokens=1024)\n",
    "    return tokenizer.decode(*outputs\n",
    "    , skip_special_tokens=True)\n",
    "\n",
    "prompt = \"å‘å‰èµ°1ç±³\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False  # ç¦ç”¨ Qwen3 çš„ thinking æ¨¡å¼\n",
    ")\n",
    "response = generate(text)\n",
    "print(\"-\"*80)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5cf7d4",
   "metadata": {
    "id": "1b5cf7d4"
   },
   "source": [
    "# Clean GPU Memory and save lora zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d346e523",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:28:54.926149Z",
     "iopub.status.busy": "2025-11-19T01:28:54.925914Z",
     "iopub.status.idle": "2025-11-19T01:28:55.965599Z",
     "shell.execute_reply": "2025-11-19T01:28:55.964997Z",
     "shell.execute_reply.started": "2025-11-19T01:28:54.926132Z"
    },
    "id": "d346e523",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Empty VRAM\n",
    "del model\n",
    "del trainer\n",
    "import gc\n",
    "gc.collect()\n",
    "gc.collect()\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import datetime\n",
    "\n",
    "def file2zip(packagePath, zipPath):\n",
    "    '''\n",
    "  :param packagePath: æ–‡ä»¶å¤¹è·¯å¾„\n",
    "  :param zipPath: å‹ç¼©åŒ…è·¯å¾„\n",
    "  :return:\n",
    "  '''\n",
    "    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n",
    "    for path, dirNames, fileNames in os.walk(packagePath):\n",
    "        fpath = path.replace(packagePath, '')\n",
    "        for name in fileNames:\n",
    "            fullName = os.path.join(path, name)\n",
    "            name = fpath + '\\\\' + name\n",
    "            zip.write(fullName, name)\n",
    "    zip.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # æ–‡ä»¶å¤¹è·¯å¾„\n",
    "    packagePath = '/kaggle/working/lora_models/qwen3_1_7b_custom'\n",
    "    zipPath = '/kaggle/working/output.zip'\n",
    "    if os.path.exists(zipPath):\n",
    "        os.remove(zipPath)\n",
    "    file2zip(packagePath, zipPath)\n",
    "    print(\"æ‰“åŒ…å®Œæˆ\")\n",
    "    print(datetime.datetime.utcnow())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f474353",
   "metadata": {
    "id": "9f474353"
   },
   "source": [
    "## Load the trained model back and integrate the trained LoRA within."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8502540",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T01:28:55.966745Z",
     "iopub.status.busy": "2025-11-19T01:28:55.966480Z",
     "iopub.status.idle": "2025-11-19T01:29:31.532373Z",
     "shell.execute_reply": "2025-11-19T01:29:31.531508Z",
     "shell.execute_reply.started": "2025-11-19T01:28:55.966724Z"
    },
    "id": "b8502540",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from peft import PeftModel\n",
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "# import os\n",
    "\n",
    "# # æ˜¾å­˜ç›‘æ§å‡½æ•°\n",
    "# def get_memory_usage():\n",
    "#     \"\"\"è·å–å½“å‰GPUæ˜¾å­˜ä½¿ç”¨æƒ…å†µï¼ˆMBï¼‰\"\"\"\n",
    "#     if torch.cuda.is_available():\n",
    "#         allocated = torch.cuda.memory_allocated() / 1024**2  # MB\n",
    "#         reserved = torch.cuda.memory_reserved() / 1024**2   # MB\n",
    "#         return allocated, reserved\n",
    "#     return 0, 0\n",
    "\n",
    "# def format_memory(mb):\n",
    "#     \"\"\"æ ¼å¼åŒ–æ˜¾å­˜æ˜¾ç¤º\"\"\"\n",
    "#     if mb < 1024:\n",
    "#         return f\"{mb:.2f} MB\"\n",
    "#     else:\n",
    "#         return f\"{mb/1024:.2f} GB\"\n",
    "\n",
    "# # åˆå§‹åŒ–æ˜¾å­˜ç›‘æ§\n",
    "# torch.cuda.reset_peak_memory_stats() if torch.cuda.is_available() else None\n",
    "# memory_stats = {\n",
    "#     \"initial\": get_memory_usage(),\n",
    "#     \"after_tokenizer\": None,\n",
    "#     \"after_base_model\": None,\n",
    "#     \"after_resize\": None,\n",
    "#     \"after_lora\": None,\n",
    "#     \"after_merge\": None,\n",
    "#     \"final\": None\n",
    "# }\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ“Š GPU æ˜¾å­˜ç›‘æ§\")\n",
    "# print(\"=\" * 80)\n",
    "# allocated, reserved = memory_stats[\"initial\"]\n",
    "# print(f\"åˆå§‹æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# # Re-define quantization parameters if needed, or use the ones defined earlier\n",
    "# bnb_4bit_quant_type = \"nf4\" # or \"fp4\"\n",
    "# use_nested_quant = True # or False\n",
    "\n",
    "# model_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# # Get the cache directory from environment variable\n",
    "# transformers_cache_dir = os.environ.get('TRANSFORMERS_CACHE', '')\n",
    "\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=use_nested_quant,\n",
    "#     bnb_4bit_quant_type=bnb_4bit_quant_type,\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "# # Load trained LoRA model\n",
    "# output_path = \"/kaggle/working/lora_models/qwen3_1_7b_custom\"\n",
    "# print(f\"\\nğŸ“‚ åŠ è½½ LoRA æ¨¡å‹: {output_path}\")\n",
    "\n",
    "# # ğŸ”§ æ­¥éª¤1: å…ˆåŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„tokenizerï¼ˆç¡®ä¿è¯æ±‡è¡¨å¤§å°ä¸€è‡´ï¼‰\n",
    "# print(\"\\nğŸ”„ åŠ è½½è®­ç»ƒæ—¶çš„ tokenizer...\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(output_path, padding_side=\"left\")\n",
    "# print(f\"   âœ… Tokenizer åŠ è½½å®Œæˆï¼Œè¯æ±‡è¡¨å¤§å°: {len(tokenizer)}\")\n",
    "\n",
    "# # Fix: Set pad_token to avoid attention_mask warnings\n",
    "# if tokenizer.pad_token is None:\n",
    "#     tokenizer.pad_token = tokenizer.eos_token\n",
    "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "#     print(f\"   è®¾ç½® pad_token = eos_token ({tokenizer.pad_token})\")\n",
    "\n",
    "# memory_stats[\"after_tokenizer\"] = get_memory_usage()\n",
    "# allocated, reserved = memory_stats[\"after_tokenizer\"]\n",
    "# print(f\"   ğŸ’¾ æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# # ğŸ”§ æ­¥éª¤2: åŠ è½½åŸºç¡€æ¨¡å‹\n",
    "# print(\"\\nğŸ”„ åŠ è½½åŸºç¡€æ¨¡å‹...\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\n",
    "#     model_id, \n",
    "#     quantization_config=bnb_config,\n",
    "#     device_map={\"\":0},\n",
    "#     cache_dir=transformers_cache_dir\n",
    "# )\n",
    "\n",
    "# memory_stats[\"after_base_model\"] = get_memory_usage()\n",
    "# allocated, reserved = memory_stats[\"after_base_model\"]\n",
    "# print(f\"   ğŸ’¾ æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# # ğŸ”§ æ­¥éª¤3: Resizeæ¨¡å‹è¯æ±‡è¡¨ä»¥åŒ¹é…è®­ç»ƒæ—¶çš„tokenizer\n",
    "# print(f\"\\nğŸ”„ Resizeæ¨¡å‹è¯æ±‡è¡¨åˆ°: {len(tokenizer)}\")\n",
    "# model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# memory_stats[\"after_resize\"] = get_memory_usage()\n",
    "# allocated, reserved = memory_stats[\"after_resize\"]\n",
    "# print(f\"   ğŸ’¾ æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# # ğŸ”§ æ­¥éª¤4: åŠ è½½LoRAé€‚é…å™¨\n",
    "# print(\"\\nğŸ”„ åŠ è½½ LoRA é€‚é…å™¨...\")\n",
    "# model = PeftModel.from_pretrained(model, output_path)\n",
    "\n",
    "# memory_stats[\"after_lora\"] = get_memory_usage()\n",
    "# allocated, reserved = memory_stats[\"after_lora\"]\n",
    "# print(f\"   ğŸ’¾ æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# print(\"\\nğŸ”„ åˆå¹¶ LoRA æƒé‡...\")\n",
    "# model = model.merge_and_unload()\n",
    "# model.config.max_length = 512\n",
    "# model.eval()\n",
    "\n",
    "# memory_stats[\"after_merge\"] = get_memory_usage()\n",
    "# allocated, reserved = memory_stats[\"after_merge\"]\n",
    "# print(f\"   ğŸ’¾ æ˜¾å­˜: å·²åˆ†é… {format_memory(allocated)}, å·²ä¿ç•™ {format_memory(reserved)}\")\n",
    "\n",
    "# memory_stats[\"final\"] = get_memory_usage()\n",
    "\n",
    "# # è·å–å³°å€¼æ˜¾å­˜\n",
    "# if torch.cuda.is_available():\n",
    "#     peak_allocated = torch.cuda.max_memory_allocated() / 1024**2  # MB\n",
    "#     peak_reserved = torch.cuda.max_memory_reserved() / 1024**2     # MB\n",
    "# else:\n",
    "#     peak_allocated = 0\n",
    "#     peak_reserved = 0\n",
    "\n",
    "# print(\"\\n\" + \"=\" * 80)\n",
    "# print(\"ğŸ“Š æ˜¾å­˜ä½¿ç”¨ç»Ÿè®¡\")\n",
    "# print(\"=\" * 80)\n",
    "# print(f\"åˆå§‹æ˜¾å­˜:     å·²åˆ†é… {format_memory(memory_stats['initial'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['initial'][1])}\")\n",
    "# if memory_stats[\"after_tokenizer\"]:\n",
    "#     print(f\"åŠ è½½Tokenizerå: å·²åˆ†é… {format_memory(memory_stats['after_tokenizer'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['after_tokenizer'][1])}\")\n",
    "# if memory_stats[\"after_base_model\"]:\n",
    "#     print(f\"åŠ è½½åŸºç¡€æ¨¡å‹å: å·²åˆ†é… {format_memory(memory_stats['after_base_model'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['after_base_model'][1])}\")\n",
    "# if memory_stats[\"after_resize\"]:\n",
    "#     print(f\"Resizeè¯æ±‡è¡¨å: å·²åˆ†é… {format_memory(memory_stats['after_resize'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['after_resize'][1])}\")\n",
    "# if memory_stats[\"after_lora\"]:\n",
    "#     print(f\"åŠ è½½LoRAé€‚é…å™¨å: å·²åˆ†é… {format_memory(memory_stats['after_lora'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['after_lora'][1])}\")\n",
    "# if memory_stats[\"after_merge\"]:\n",
    "#     print(f\"åˆå¹¶LoRAæƒé‡å: å·²åˆ†é… {format_memory(memory_stats['after_merge'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['after_merge'][1])}\")\n",
    "# print(f\"æœ€ç»ˆæ˜¾å­˜:     å·²åˆ†é… {format_memory(memory_stats['final'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['final'][1])}\")\n",
    "# print(\"-\" * 80)\n",
    "# print(f\"ğŸ”¥ å³°å€¼æ˜¾å­˜ä½¿ç”¨: å·²åˆ†é… {format_memory(peak_allocated)}, å·²ä¿ç•™ {format_memory(peak_reserved)}\")\n",
    "# print(f\"ğŸ“ˆ æ˜¾å­˜å¢é•¿: å·²åˆ†é… {format_memory(memory_stats['final'][0] - memory_stats['initial'][0])}, å·²ä¿ç•™ {format_memory(memory_stats['final'][1] - memory_stats['initial'][1])}\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# print(\"\\nâœ… æ¨¡å‹åŠ è½½å®Œæˆï¼ˆè¯æ±‡è¡¨å¤§å°å·²åŒ¹é…ï¼‰\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a05b9b7",
   "metadata": {
    "id": "2a05b9b7"
   },
   "source": [
    "## Answer generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4c3ad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T02:37:49.828285Z",
     "iopub.status.busy": "2025-11-19T02:37:49.828034Z",
     "iopub.status.idle": "2025-11-19T02:38:49.504640Z",
     "shell.execute_reply": "2025-11-19T02:38:49.503905Z",
     "shell.execute_reply.started": "2025-11-19T02:37:49.828269Z"
    },
    "id": "4e4c3ad5",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# @torch.no_grad()\n",
    "# def generate(query_list, return_answer: bool = False):\n",
    "#     def conv_format(query):\n",
    "#         conv = get_conv_template('phoenix')\n",
    "#         conv.append_message(conv.roles[0], query)\n",
    "#         conv.append_message(conv.roles[1], None)\n",
    "#         return conv.get_prompt()\n",
    "#     query_list = [conv_format(query) for query in query_list]\n",
    "#     input_ids = tokenizer(query_list, padding=True, truncation=True, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(\"cuda\")\n",
    "#     n_input, n_seq = input_ids.shape[0], input_ids.shape[-1]\n",
    "#     output_ids = []\n",
    "#     step = 1\n",
    "#     for index in tqdm(range(0, n_input, step)):\n",
    "#         outputs = model.generate(\n",
    "#             input_ids=input_ids[index: min(n_input, index+step)],\n",
    "#             do_sample=False,\n",
    "#             max_new_tokens=512,\n",
    "#             # temperature=0.3,\n",
    "#             repetition_penalty=1.0,\n",
    "#         )\n",
    "#         output_ids += outputs\n",
    "#     responses = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "#     if return_answer:\n",
    "#         return [response[len(query):].strip() for query, response in zip(query_list, responses)]\n",
    "#     return responses\n",
    "\n",
    "# # test\n",
    "# print(\"\\n\".join(generate([\"What's the weather like today?\", \"Who are you?\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dK-NHBIVce90",
   "metadata": {
    "id": "dK-NHBIVce90"
   },
   "source": [
    "## Model Inference\n",
    "\n",
    "Test the trained model with sample queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UpU2_tmDcmT2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T02:42:18.837579Z",
     "iopub.status.busy": "2025-11-19T02:42:18.837286Z",
     "iopub.status.idle": "2025-11-19T02:42:18.868466Z",
     "shell.execute_reply": "2025-11-19T02:42:18.867848Z",
     "shell.execute_reply.started": "2025-11-19T02:42:18.837560Z"
    },
    "id": "UpU2_tmDcmT2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Test the model with sample queries\n",
    "# test_queries = [\n",
    "#     \"å‘å³è½¬15åº¦ï¼Œç„¶åå‘å‰èµ°0.5ç±³ã€‚\",\n",
    "#     \"ç»™æˆ‘ç«™èµ·æ¥ï¼Œç„¶åå‘å·¦è½¬45åº¦ã€‚\",\n",
    "#     \"å‘å‰ç§»åŠ¨8ç±³ï¼Œæ¥ç€å³è½¬120åº¦ï¼Œå†èµ°3ç±³ã€‚\"\n",
    "# ]\n",
    "\n",
    "# print(\"=\" * 80)\n",
    "# print(\"ğŸ§ª æµ‹è¯•æ¨¡å‹æ¨ç†...\")\n",
    "# print(\"=\" * 80)\n",
    "\n",
    "# test_responses = generate(test_queries, return_answer=True)\n",
    "\n",
    "# for i, (query, response) in enumerate(zip(test_queries, test_responses)):\n",
    "#     print(f\"\\næŸ¥è¯¢ {i+1}:\")\n",
    "#     print(f\"  é—®é¢˜: {query}\")\n",
    "#     print(f\"  å›ç­”: {response[:200]}...\")\n",
    "#     print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bZe0JS2eRK_",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-19T02:59:52.256501Z",
     "iopub.status.busy": "2025-11-19T02:59:52.255790Z",
     "iopub.status.idle": "2025-11-19T02:59:52.262353Z",
     "shell.execute_reply": "2025-11-19T02:59:52.261709Z",
     "shell.execute_reply.started": "2025-11-19T02:59:52.256476Z"
    },
    "id": "1bZe0JS2eRK_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Save test results\n",
    "# output_file = \"/kaggle/working/lora_models/qwen3_1_7b_custom/test_results.json\"\n",
    "# test_results = [\n",
    "#     {\"query\": q, \"response\": r} \n",
    "#     for q, r in zip(test_queries, test_responses)\n",
    "# ]\n",
    "# with open(output_file, 'w', encoding='utf-8') as writer:\n",
    "#     json.dump(test_results, writer, indent=4, ensure_ascii=False)\n",
    "# print(f\"\\nâœ… æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {output_file}\")\n",
    "# print(f\"å…± {len(test_results)} æ¡æµ‹è¯•ç»“æœ\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8769458,
     "sourceId": 13777663,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 592.642275,
   "end_time": "2023-10-19T19:56:29.207434",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-10-19T19:46:36.565159",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
